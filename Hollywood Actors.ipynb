{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hollywood Actors \n",
    "###  Can we predict if an actor won the Oscar based on the career article in Wikipedia?\n",
    "\n",
    "Shmuel Naaman "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Based on your request I analyze the \"Holywood Actors\" articles in Wikipedia. I choose to use the list of actors that appears on 'Hollywood Walk of Fame motion picture stars'. This is not a complete list of actors but it represents significant actors in the Hollywood industry. The list includes around 900 actors 200 of them were nominated to Oscar and another 200 won an Oscar. \n",
    "\n",
    "I thought that building a model that classify actors that won the Oscar, nominated to Oscar and did not win the oscar will demonstrate some of my skills in data processing and modeling. \n",
    "\n",
    "Due to obviously limited time, I focus my efforts on efficient preprocessing that will enhance the signal. \n",
    " * I perform basic cleaning of the data \n",
    " * Used nltk sentence tokenizer as an input to create stem words.\n",
    " * Remove stop words\n",
    " * Perform word embedding as part of the deep neural network. \n",
    " * I choose to use an artificial recurrent neural network (RNN) architecture of the type Long short-term memory (LSTM). LSTM networks are well-suited to classifying, processing and making predictions based on time series data.\n",
    " \n",
    "These steps allow me to reduce the dimension of the problem, and work with relatively clean data. Which is particularly important for this data set which is quite small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import wikipedia as wp\n",
    "import tensorflow as tf \n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Optimally these functions would be in a separate file. However, since they include important steps of the analysis I decided to present them here with the rest of the analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    " * Replace punctuation with tokens so we can use them in our model \n",
    " * Remove the word 'Oscar' from the article\n",
    " * Remove the words 'section' and 'Career' since they are related with the articale and not the actor\n",
    "There are many other preprocessing that worth considering here for example the work 'nomination' \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model \n",
    "    # We remove the word 'Oscar' from the article\n",
    "    # We did remove the words 'section' and 'Career' since they are related with the articale and not the actor\n",
    "    # There are many other preprocessing that worth considering here for example the work 'nomination' \n",
    "    # We want to keep it simple and at the same time to demonstrate the analysis\n",
    "     \n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace('career', ' ')\n",
    "    text = text.replace('oscar', ' ')\n",
    "    text = text.replace('sections', ' ')\n",
    "    text = text.replace('section', ' ')\n",
    "\n",
    "\n",
    "    return  text\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the words to roots\n",
    " Replace all words by the root words. \n",
    " * That will simplify the dataset by reducing the dimention of the data. \n",
    " * At the same time, considering the size of the data set, having all the variation of a key words as a single category might strength the signal.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shmuel\n",
      "[nltk_data]     Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shmuel\n",
      "[nltk_data]     Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shmuel Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Shmuel Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# Create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Stem words\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    # Break the paragraph into sentences\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "     \n",
    "    for sentence in tokenized_sent:\n",
    " \n",
    "        # Seperating sentences to words\n",
    "        tokenized_word = word_tokenize(sentence)\n",
    "         \n",
    "        # Identify nouns, verbs, adjectives, and adverbs.\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        \n",
    "        # Clasify to root word Text Normalization \"performing\" -> \"perform\"\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        \n",
    "        # adding all the lists into a single list\n",
    "        lemmatized_list.extend(lemmatized)\n",
    " \n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import list of Hollywood Actors names. \n",
    "Wikipedia: \"List of actors with Hollywood Walk of Fame motion picture stars\".\n",
    "\n",
    "I used the 'wikipedia' library which is awesome and save a lot of work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the html source\n",
    "html = wp.page(\"List_of_actors_with_Hollywood_Walk_of_Fame_motion_picture_stars\").html().encode(\"UTF-8\")\n",
    "\n",
    " \n",
    "#  find the table element, does the parsing and creates a DataFrame\n",
    "df = pd.read_html(html)[1]\n",
    "\n",
    "# Replace the header with the first row\n",
    "new_header = df.iloc[0] \n",
    "df = df[1:]  \n",
    "df.columns = new_header \n",
    "\n",
    "# ading a column for the careere \n",
    "df['career']='a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899 entries, 1 to 899\n",
      "Data columns (total 10 columns):\n",
      "Actor       899 non-null object\n",
      "nan         899 non-null object\n",
      "Born        899 non-null object\n",
      "Died        899 non-null object\n",
      "Age         899 non-null object\n",
      "Address     899 non-null object\n",
      "Inducted    899 non-null object\n",
      "At age      899 non-null object\n",
      "Oscar       899 non-null object\n",
      "career      899 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oscar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oscar</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nom</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Won</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~</th>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0      Oscar\n",
       "Oscar       \n",
       "Nom      173\n",
       "Won      176\n",
       "~        550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "df.groupby([ 'Oscar'])[['Oscar']].count().plot(kind='bar').set_ylabel('Count')\n",
    "df.groupby(['Oscar' ])[['Oscar']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: The 'Nomination' (Nom) and 'Won' are balanced but many actors are not in either of these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import the actors 'Career' article. \n",
    " * Here we use \"wikipediaapi\" library to extract the 'Career' section for each actor article that appears in the list.  \n",
    " * It is important to note here that the preprocessing is perform as we reading the data. Hopefully, that will save processing time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nltk sentance tokenizer. \n",
    "* tag words. \n",
    "* root on the original text. stanford nlp library. \n",
    "* stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the Career for each actor and adding them to the dataframe\n",
    " \n",
    "all_text=str([])\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "for ind, actor_n in  enumerate(df.Actor):\n",
    "    # Uploading the actor article.\n",
    "    page_py = wiki_wiki.page(actor_n)\n",
    "    # Extracting the \"Career\" section and preprocessing the text.\n",
    "    #example_sent = preprocess(lemmatize_doc(str(page_py.section_by_title('Career')))) \n",
    "    example_sent = preprocess(lemmatize_doc(str(page_py.summary)))     \n",
    "     \n",
    "    # Stop words\n",
    "    section_py = ' '.join([w for w in example_sent.split() if not w in stop_words])    \n",
    "    # Processed \"Career\" section saved into a \"bag of words\" for the encoding \n",
    "    all_text +=  \" \"+section_py\n",
    "    \n",
    "    # Procesed \"Career\" section added to the dataframe\n",
    "    df['career'].iloc[ind] = section_py\n",
    "# \"bag of words\"\n",
    "words = all_text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 11576\n",
      "embedding_dimensions = 10.372647745416891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[]',\n",
       " 'william',\n",
       " 'alexander',\n",
       " '``',\n",
       " 'bud',\n",
       " \"''\",\n",
       " 'abbott',\n",
       " '<LEFT_PAREN>',\n",
       " 'october',\n",
       " '2',\n",
       " '<COMMA>',\n",
       " '1895',\n",
       " '–',\n",
       " 'april',\n",
       " '24',\n",
       " '<COMMA>',\n",
       " '1974',\n",
       " '<RIGHT_PAREN>',\n",
       " 'american',\n",
       " 'actor',\n",
       " '<COMMA>',\n",
       " 'best',\n",
       " 'know',\n",
       " 'film',\n",
       " 'comedy',\n",
       " 'double',\n",
       " 'act',\n",
       " '<COMMA>',\n",
       " 'straight',\n",
       " 'man']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del all_text\n",
    "print('Number of unique words :',len(set(words)))\n",
    "print('embedding_dimensions =',  len(set(words))**0.25)\n",
    "words[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words =pd.DataFrame(words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "Using one hot encoding to labels categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [  'Nom','won','non' ]\n",
    "labels =    pd.DataFrame(  columns=columns)\n",
    "labels[[ 'Nom','won','non']] =  pd.DataFrame(pd.get_dummies(df['Oscar']))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>won</th>\n",
       "      <th>non</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nom  won  non\n",
       "1    0    0    1\n",
       "2    0    0    1\n",
       "3    1    0    0\n",
       "4    0    0    1\n",
       "5    0    0    1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words.to_csv('words.csv' ,index=False)\n",
    "#df.to_csv('holly_actr.csv' )\n",
    "#labels.to_csv('labels.csv' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_words = pd.read_csv('words.csv') \n",
    "#words = df_words['0'].values.tolist()\n",
    "#df = pd.read_csv('holly_actr.csv') \n",
    "#labels = pd.read_csv('labels.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "Each unique word in the 'bag of words' gets an integer. That will be used to encode the text in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of each word in the text.\n",
    "counts = Counter(words)\n",
    "# sort the words acording to the frequency they appears\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "# transfor each word to a unique integer.\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    " \n",
    "#Transform the 'career' into list of integers acording to the encoding.\n",
    "career_ints = []\n",
    "for i, each in enumerate(df['career']):\n",
    "     \n",
    "    if (each !='none' and each !='a' and  each !='nan'):\n",
    "        career_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "    else:\n",
    "        career_ints.append([0])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview for the career articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 word-length reviews: 0\n",
      "Maximum review length: 584\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in career_ints])\n",
    "print(\"1 word-length reviews: {}\".format(review_lens[1]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that around half of the actors have no articles. That might be due to a different section name. In some cases, the 'early life' section is combined with the 'career'. This is something that can be solved. However, the data include enough articles to demonstrate the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>123.933259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>102.893809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  899.000000\n",
       "mean   123.933259\n",
       "std    102.893809\n",
       "min      0.000000\n",
       "25%     40.000000\n",
       "50%     91.000000\n",
       "75%    182.000000\n",
       "max    584.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "#pd.DataFrame.from_dict(  review_lens ) \n",
    "review_le = ([len(x) for x in career_ints])\n",
    "review_df = pd.DataFrame(review_le)\n",
    "\n",
    "review_df.hist(bins = 20) \n",
    "pl.xlabel('Number of words')\n",
    "pl.ylabel('Number of samples')\n",
    "pl.title('distribution of Number of words in articles')\n",
    "pd.DataFrame(review_df.describe())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2: 50% of the articles are shorter than 150 words. The average number of words in the articles is 566 words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>123.933259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>102.893809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  899.000000\n",
       "mean   123.933259\n",
       "std    102.893809\n",
       "min      0.000000\n",
       "25%     40.000000\n",
       "50%     91.000000\n",
       "75%    182.000000\n",
       "max    584.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "#pd.DataFrame.from_dict(  review_lens ) \n",
    "review_le =  ([len(x) for x in career_ints])\n",
    "review_df = pd.DataFrame(review_le)\n",
    "\n",
    "review_df[(review_df[0]!=1)&(review_df[0]<3000)].hist(bins = 50) \n",
    "pl.xlabel('Number of words')\n",
    "pl.ylabel('Number of samples')\n",
    "pl.title('distribution of Number of words in articles')\n",
    "pd.DataFrame(review_df[review_df[0]!=1].describe())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_df[review_df[0]!=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles that have career section and the actor won or was nominated to Oscar: 349\n"
     ]
    }
   ],
   "source": [
    "samp = len(df[ (df['Oscar'] != '~') & (df['career'] != 'none') ])\n",
    "print(\"Number of articles that have career section and the actor won or was nominated to Oscar: {}\".format(samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for the model\n",
    "Create an array of features that contains the data we'll pass to the network. The data come from 'career_ints', since we want to feed integers to the network. Each row is 300 words long. The shorter rows are than pads with 0s. For the long rows, only the first 300 words are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0) into shape (300)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b1a0c32db151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcareer_ints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcareer_ints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (0) into shape (300)"
     ]
    }
   ],
   "source": [
    "# Considering the distribution with no 0, perhaps seq_len of 800 is more appropriate \n",
    "import numpy as np\n",
    "seq_len = 300\n",
    "features = np.zeros((len(career_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(career_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    3,   20,    4],\n",
       "       [   0,    0,    0, ...,    0,    0,    0],\n",
       "       [   3,   63,    4, ..., 4536, 1863,    1],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    3,   20,    4],\n",
       "       [   0,    0,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(719, 800) \n",
      "Validation set: \t(90, 800) \n",
      "Test set: \t\t(90, 800) \n",
      "Test set: \t\t(90, 3) \n",
      "Validation set: \t(90, 3) \n",
      "Train set: \t\t(719, 3)\n"
     ]
    }
   ],
   "source": [
    "#Can be replace with train test split\n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels.values[:split_idx ], labels.values[split_idx: ]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_y.shape),\n",
    "     \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "     \"\\nTrain set: \\t\\t{}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph\n",
    "\n",
    "### Hyperparameters Defining.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. \n",
    "* `lstm_layers`: Number of LSTM layers in the network.\n",
    "* `batch_size`: The number of articles to feed the network in one training pass.  \n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 90\n",
    "learning_rate = 0.001\n",
    "NUM_CLASSES = 3\n",
    "epochs = 5\n",
    "embed_size = 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in 300 words long vectors. Each batch will be batch_size vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. We need to do this because it is inefficient to one-hot encode our classes here. Instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. It is also possible to train an embedding layer using word2vec, then load it here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "# embed_size = 100 # can be other size. \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell\n",
    "\n",
    " \n",
    "Next, we'll create our LSTM cells to use in the recurrent network ([TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)). \n",
    "\n",
    "To create a basic LSTM cell for the graph, we'll   use `tf.contrib.rnn.BasicLSTMCell`.\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)\n",
    "```\n",
    "\n",
    "BasicLSTMCell   takes a parameter `num_units`, the number of units in the cell (`lstm_size`).  \n",
    "\n",
    " \n",
    "\n",
    "Next, we add dropout to the cell with `tf.contrib.rnn.DropoutWrapper`. This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make the network better with almost no effort! \n",
    "\n",
    "\n",
    "The network will have better performance with more layers. Adding more layers allows the network to learn complex relationships. A simple way to create multiple layers of LSTM cells with `tf.contrib.rnn.MultiRNNCell`:\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "```\n",
    "\n",
    "Here, `[drop] * lstm_layers` creates a list of cells (`drop`) that is `lstm_layers` long. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "So the final cell we're using in the network is actually multiple  LSTM cells with dropout. But it all works the same from an architectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Basic LSTM cell\n",
    " \n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    " \n",
    "    # Dropout to the cell\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN forward pass\n",
    "Now we need to actually run the data through the RNN nodes. We use tf.nn.dynamic_rnn to do this. We pass in the RNN cell you created (our multiple layered LSTM cell for instance), and the inputs to the network.\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "Above I created an initial state, initial_state, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. tf.nn.dynamic_rnn takes care of most of the work for us. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "#### Training Cost\n",
    "We only care about the final output, we'll be using that as our prediction. So we need to grab the last output with `outputs[:, -1]`, the calculate the cost from that and `labels_`. Since we have more than 2 labels it is important to use the softmax_cross_entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], NUM_CLASSES, \n",
    "                                                    activation_fn=tf.sigmoid)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_, logits=predictions))\n",
    "    #cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy\n",
    "Here we   calculate the accuracy which I'll use in the validation pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "This function returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the x and y arrays and returns slices out of those arrays with size [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=batch_size):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "lstm_size = 256\n",
    "num_units = [128, 64 ]\n",
    "lstm_layers = 2\n",
    "batch_size = 30\n",
    "learning_rate = 0.001\n",
    "NUM_CLASSES = 3\n",
    "epochs = 3\n",
    "keep_prob = 0.5\n",
    "embed_size = 50  \n",
    "seq_len = 800\n",
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "with graph.as_default():\n",
    "    # Basic LSTM cell\n",
    " \n",
    "    lstm = [ tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(num_units=n), \n",
    "                                           output_keep_prob=keep_prob) for n in num_units]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(lstm)\n",
    "  \n",
    " \n",
    "\n",
    "    initial_state = cell.zero_state(  batch_size , tf.float32)\n",
    "# RNN forward pass\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)\n",
    "    \n",
    "# Output Training Cost\n",
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], NUM_CLASSES, \n",
    "                                                    activation_fn=tf.sigmoid)\n",
    "    # For multi output\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_, logits=predictions))\n",
    "    # For binarry output \n",
    "    # cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "# Validation, \n",
    "with graph.as_default():\n",
    "    # For binarry output \n",
    "    # correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # For multi output  \n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels_, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n",
    "    \n",
    "# Batching \n",
    "def get_batches(x, y, batch_size=batch_size):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below is the training code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Iteration: 5 Train loss: 0.920\n",
      "Epoch: 0/3 Iteration: 10 Train loss: 0.885\n",
      "Epoch: 0/3 Iteration: 15 Train loss: 1.127\n",
      "Epoch: 0/3 Iteration: 20 Train loss: 0.989\n",
      "Epoch: 1/3 Iteration: 25 Train loss: 0.889\n",
      "Val acc: 0.589\n",
      "Epoch: 1/3 Iteration: 30 Train loss: 0.920\n",
      "Epoch: 1/3 Iteration: 35 Train loss: 0.985\n",
      "Epoch: 1/3 Iteration: 40 Train loss: 0.855\n",
      "Epoch: 1/3 Iteration: 45 Train loss: 0.859\n",
      "Epoch: 2/3 Iteration: 50 Train loss: 0.854\n",
      "Val acc: 0.589\n",
      "Epoch: 2/3 Iteration: 55 Train loss: 1.117\n",
      "Epoch: 2/3 Iteration: 60 Train loss: 1.049\n",
      "Epoch: 2/3 Iteration: 65 Train loss: 0.979\n"
     ]
    }
   ],
   "source": [
    " \n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, :],\n",
    "                    keep_prob: 0.7,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:,:],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "             \n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar LSTM implemented using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 800, 50)           1492500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 800, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 800, 128)          91648     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,633,751\n",
      "Trainable params: 1,633,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 575 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "575/575 [==============================] - 135s 235ms/step - loss: 1.0476 - acc: 0.5774 - val_loss: 0.9935 - val_acc: 0.6528\n",
      "Epoch 2/3\n",
      "575/575 [==============================] - 127s 220ms/step - loss: 0.9725 - acc: 0.6157 - val_loss: 0.8843 - val_acc: 0.6528\n",
      "Epoch 3/3\n",
      "575/575 [==============================] - 129s 225ms/step - loss: 0.9415 - acc: 0.6139 - val_loss: 0.8827 - val_acc: 0.6528\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    " \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "lstm_size = 256\n",
    "num_units = [128, 64 ]\n",
    "lstm_layers = 2\n",
    "batch_size = 30\n",
    "learning_rate = 0.001\n",
    "NUM_CLASSES = 3\n",
    "epochs = 3\n",
    "embed_size = 50  \n",
    "seq_len = 800\n",
    " \n",
    "n_most_common_words=29850 \n",
    " \n",
    "#model.add(LSTM(128, dropout=keep_prob, recurrent_dropout=keep_prob,return_sequences=True))\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words,50, input_length=800))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(128, dropout=0.7, recurrent_dropout=0.7,return_sequences=True)) \n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7 ))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "history = model.fit(train_x, train_y, epochs=3, batch_size=30,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test accuracy: 0.544\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, :],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single case\n",
    "We can enter a actor name and the model will categorize whather it win was nominated or none. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_n='Fay Bainter'\n",
    "page_py = wiki_wiki.page(actor_n)\n",
    "# Extracting the \"Career\" section and preprocessing the text.\n",
    "example_sent = preprocess(lemmatize_doc(str(page_py.section_by_title('Career')))) \n",
    "# Stop words\n",
    "section_py = ' '.join([w for w in example_sent.split() if not w in stop_words])    \n",
    "\n",
    "each = section_py\n",
    "\n",
    "career_ints = []\n",
    "\n",
    "if (each !='none' and each !='a' and  each !='nan'):\n",
    "    career_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "else:\n",
    "    career_ints.append([0])\n",
    "\n",
    "features = np.zeros((len(career_ints), seq_len), dtype=int)\n",
    "row = career_ints\n",
    "\n",
    "for i, row in enumerate(career_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "samp = features[0,:].reshape(1,seq_len)\n",
    "test_x1=test_x[0:30]\n",
    "test_x1[0]=samp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Fay Bainter Nan\n"
     ]
    }
   ],
   "source": [
    " \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "     \n",
    " \n",
    "    feed = {inputs_: test_x1, \n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "    pred1  = sess.run([predictions ], feed_dict=feed)\n",
    " \n",
    "    print(actor_n,['Nom','Won','Nan'][np.where(pred1[0][0]==np.max(pred1[0][0]))[0][0]] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflection\n",
    "\n",
    "The model performs better than I expected, I find that somehow suspicious! I would like to find out what are the main reasons for that high performance.  \n",
    "\n",
    "Optional sources that should be examined:  \n",
    " * The length of the article. I suspect that Oscar nomination and Oscar holders will have longer text. \n",
    " * The word 'nomination' (or other words) with all its variation might bias the text. \n",
    " * The metric that we choose together with the unbalanced labels might bias the results. The metric considers the performance of the model for the 3 categories. Perhaps a more focus metric will show a different result. \n",
    " * Comparing the performance to some baseline. For example, What will be the accuracy if all prediction is \"not nominated and not won\". \n",
    " \n",
    "### Improvements \n",
    " * The preprocessing was quite basic, there are other approaches for data preprocessing that can be used. \n",
    " * Larger data set will allow us to use the actual words instead of the roots. \n",
    " * Not all the possible articles where uploaded. Better data collection will increase the information in the dataset. \n",
    " * The model was quite simple, we can increase the number of hidden layers and the number of neurons. We can use other models.  \n",
    " * The cleaning of the data can be more sophisticated. We can easily remove words with very high or very low frequency. Some of the words in the articles are related to numbering the sections that can be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "#### # Extracting the names of actors and the oscar nomination :  https://qxf2.com/blog/web-scraping-using-python/ , https://pypi.org/project/wikipedia/\n",
    "\n",
    "#### # Extracting the career : https://pypi.org/project/Wikipedia-API/0.2.0/\n",
    "#### # Model : https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
