{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hollywood Actors \n",
    "###  Can we predict if an actor won the Oscar based on the career article in Wikipedia?\n",
    "\n",
    "Shmuel Naaman "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods used for the analysis\n",
    "\n",
    "###  LSTM, Topic Extraction, Cosine Similarities, TfIdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "#### Since the performance of the naive model is not satisfactory, here I try another approach that is more elegant. \n",
    "\n",
    "The main reason for the low performance of the naive model is the multi topics in the \"Holywood Actors\" Wikipedia articles. Some of the topics are not relevant to the target variable. Therefore introduces noise and reduce the performance. \n",
    "\n",
    "Here I try a different approach to improve the performance. The idea is to extract and identifies the main topics in the articles using unsupervised learning. That will allow us to choose only relevant input for the model.  \n",
    "\n",
    "I use the same list of actors that appears on 'Hollywood Walk of Fame motion picture stars' to choose articles. This is not a complete list but it includes most of the significant actors in the Hollywood industry. I also improve the articles extraction so most of the actors have articles. The list includes around 900 actors 200 of them were nominated to Oscar and another 350 won or nominated to an Oscar. \n",
    "\n",
    "\n",
    "#### The model classifies actors that won or nominated to the Oscar as one category and the other actors as the second category.\n",
    "\n",
    "#### Overview: \n",
    "\n",
    "I focus my efforts on efficient but basic preprocessing that will enhance the signal. \n",
    " * I perform basic cleaning of the data, remove numbers, stop words, tokenize words.\n",
    " * Latent Dirichlet allocation (LDA) and Non-Negative Matrix Factorization (NMF) is implemented to identifies 5 different topics in the articles.  \n",
    " * TfIdf and Cosine Similarities are used to identifies the 4 sentences in each document that are most relevant to the target variable \"Oscar\". \n",
    " * Only these 4 sentences will be used as input for the model. \n",
    " * I choose to use an artificial recurrent neural network (RNN) architecture of the type Long short-term memory (LSTM). LSTM networks are well-suited to classifying, processing and making predictions based on time series data.\n",
    " \n",
    "These steps allow me to reduce the dimension of the problem and obtain better results despite the relatively small dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "import wikipedia as wp\n",
    "import tensorflow as tf \n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Optimally these functions would be in a separate file. However, since they include important steps of the analysis I decided to present them here with the rest of the analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    " * Remove the words 'section', 'subsections' and 'Career' since they are related to the article and not the actor. \n",
    " * The years in the articles were found to have a strong signal for the unsupervised learning, therefore, were removed.  \n",
    " * Use word_tokenize to reduce the dimension of the problem and strength the signal.  \n",
    " * That will simplify the dataset by reducing the dimension of the data. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model \n",
    "    # We remove the word 'Oscar' from the article\n",
    "    # We did remove the words 'section' and 'Career' since they are related with the articale and not the actor\n",
    "    # There are many other preprocessing that worth considering here for example the work 'nomination' \n",
    "    # We want to keep it simple and at the same time to demonstrate the analysis\n",
    "    # section subsections career\n",
    " \n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('section', ' ')\n",
    "    text = text.replace('0', ' ')\n",
    "    text = text.replace('1', ' ')\n",
    "    text = text.replace('2', ' ')\n",
    "    text = text.replace('3', ' ')\n",
    "    text = text.replace('4', ' ')\n",
    "    text = text.replace('5', ' ')\n",
    "    text = text.replace('6', ' ')\n",
    "    text = text.replace('7', ' ')\n",
    "    text = text.replace('8', ' ')\n",
    "    text = text.replace('9', ' ')\n",
    "    text = text.replace('subsections', ' ')\n",
    "    text = text.replace('career', ' ')\n",
    " \n",
    "   \n",
    "\n",
    "    return  text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shmuel\n",
      "[nltk_data]     Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shmuel\n",
      "[nltk_data]     Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shmuel Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Shmuel Naaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# Create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Stem words\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    # Break the paragraph into sentences\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "     \n",
    "    for sentence in tokenized_sent:\n",
    " \n",
    "        # Seperating sentences to words\n",
    "        tokenized_word = word_tokenize(sentence)\n",
    "         \n",
    "        # Identify nouns, verbs, adjectives, and adverbs.\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        \n",
    "        # Clasify to root word Text Normalization \"performing\" -> \"perform\"\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        \n",
    "        #lemmatized=LancasterStemmer(tagged_token)\n",
    "        \n",
    "        # adding all the lists into a single list\n",
    "        lemmatized_list.extend(lemmatized)\n",
    " \n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import list of Hollywood Actors names. \n",
    "Wikipedia page: \"List of actors with Hollywood Walk of Fame motion picture stars\".\n",
    "\n",
    "I used the ```wikipedia``` library and the ```pandas.read_html``` function which are both awesome and saves a lot of parsing work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the html source\n",
    "html = wp.page(\"List_of_actors_with_Hollywood_Walk_of_Fame_motion_picture_stars\").html().encode(\"UTF-8\")\n",
    "\n",
    "#  find the table element, does the parsing and creates a DataFrame\n",
    "df = pd.read_html(html)[1]\n",
    "\n",
    "# Replace the header with the first row\n",
    "new_header = df.iloc[0] \n",
    "df = df[1:]  \n",
    "df.columns = new_header \n",
    "\n",
    "# ading a column for the careere \n",
    "df['career']='None' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899 entries, 1 to 899\n",
      "Data columns (total 10 columns):\n",
      "Actor       899 non-null object\n",
      "nan         899 non-null object\n",
      "Born        899 non-null object\n",
      "Died        899 non-null object\n",
      "Age         899 non-null object\n",
      "Address     899 non-null object\n",
      "Inducted    899 non-null object\n",
      "At age      899 non-null object\n",
      "Oscar       899 non-null object\n",
      "career      899 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shmuel Naaman\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time', 'e']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oscar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oscar</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nom</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Won</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~</th>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0      Oscar\n",
       "Oscar       \n",
       "Nom      173\n",
       "Won      176\n",
       "~        550"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "df.groupby([ 'Oscar'])[['Oscar']].count().plot(kind='bar').set_ylabel('Count')\n",
    "df.groupby(['Oscar' ])[['Oscar']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: The 'Nomination' (Nom) and 'Won' are balanced but many actors are not in either of these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import the actors 'Career' article. \n",
    " * Here we use \"wikipediaapi\" library to extract the 'Career' section for each actor article that appears in the list.  \n",
    " * It is important to note here that the preprocessing is perform as we reading the data. Hopefully, that will save processing time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nltk sentance tokenizer. \n",
    "* tag words. \n",
    "* root on the original text. stanford nlp library. \n",
    "* stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the Career for each actor and adding them to the dataframe\n",
    " \n",
    "all_text=str([])\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "docs = []\n",
    "\n",
    "for ind, actor_n in  enumerate(df.Actor ):\n",
    "    # Uploading the actor article.\n",
    "    page_py = wiki_wiki.page(actor_n)\n",
    "    \n",
    "    # Extracting the \"Career\" section and preprocessing the text.\n",
    "    example_sent =  preprocess(lemmatize_doc(str(page_py.section_by_title('Career'))))\n",
    "    \n",
    "    # If carrer is not present choose the first available option\n",
    "    if (example_sent=='none')& (page_py.sections!=[]):\n",
    "        example_sent = preprocess(lemmatize_doc(str(page_py.section_by_title(page_py.sections[0].title))))\n",
    "    \n",
    "    # Creating the bag of words    \n",
    "    docs.append(example_sent)\n",
    "    \n",
    "    # Procesed \"Career\" section added to the dataframe\n",
    "    df['career'].iloc[ind] = example_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction\n",
    "### Latent Dirichlet allocation (LDA) and Non-Negative Matrix Factorization (NMF) is implemented to identifies 5 different topics in the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.001s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 16.898s.\n",
      "Extracting tf features for LDA...\n",
      "done in 19.193s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=2000...\n",
      "done in 0.828s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: stage broadway comedy lead production movie mgm actor act return studio york begin picture direct new york musical actress contract company\n",
      "Topic #1: school bear father early life mother family attend high school née parent high child university jewish college age brother york new york graduate\n",
      "Topic #2: award best win award best academy actress academy award nomination nominate performance supporting best supporting receive actor best actress globe golden globe academy award best golden drama\n",
      "Topic #3: episode series television season guest cbs abc nbc character tv western television series guest starred appearance portray starred cast sitcom drama appear episode\n",
      "Topic #4: million release gross comedy review worldwide gross million office box office box receive character drama video commercial action performance critic reprise role reprise\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=2000...\n",
      "done in 14.835s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: york young lead actor stage production actress movie broadway picture act begin debut return man screen new york comedy cast appearance\n",
      "Topic #1: bear school early life mother father family york parent young née son attend born high states child high school john united york city\n",
      "Topic #2: win award academy nominate best academy award supporting nomination golden best supporting award best series drama globe performance golden globe episode receive television voice\n",
      "Topic #3: television episode series run tv american abc nbc robert radio guest television series ii appearance world war ii war ii short pilot day character\n",
      "Topic #4: world say write use week company los comedy director set angeles box writer release stay los angeles interview provide year star home\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=2000...\n",
      "done in 30.919s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: young school father bear robert born family mother american age child british attend early life york new york act university parent english\n",
      "Topic #1: comedy contract studio new york york barrymore picture opposite woman silent mgm fox begin stage debut actress feature release production sign\n",
      "Topic #2: award performance best receive comedy actress drama million character release review nomination win streep cast movie success golden write critic\n",
      "Topic #3: performance actor movie say production best lead receive award success director man return love theatre york direct character war broadway\n",
      "Topic #4: series television episode award movie best actor guest comedy character tv appearance broadway musical cast performance lead direct academy voice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 2000\n",
    "n_components = 5\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    " \n",
    "data_samples = docs\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10, ngram_range=(1,3),\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=10,ngram_range=(1,3),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose Topic 2  for the NMF model (generalized Kullback-Leibler divergence) with tf-idf features\n",
    "#### Topic #2: award best win award best academy actress academy award nomination nominate performance supporting best supporting receive actor best actress globe golden globe academy award best golden drama\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the relevant sentances using cosine_similarities\n",
    "\n",
    "### Here we identify the 4 most similar sentancess to \"Topic 2\" in each article ( document) . These sentancess will be uploaded to a new column \"simila'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simila(dafr):\n",
    "    sentence= 'award best award best win academy actress academy award nomination nominate supporting performance best supporting actor receive best actress globe academy award best golden globe golden best actor'\n",
    "\n",
    "    topic_1 = tf_vectorizer.transform([sentence]) \n",
    "    cosine_similarities =linear_kernel(topic_1, tf_vectorizer.transform(sent_tokenize(dafr ))).flatten() \n",
    "    list_top = []\n",
    " \n",
    "    for ind in cosine_similarities.argsort()[:-5:-1]:\n",
    "        list_top.append(sent_tokenize(dafr )[ind])\n",
    "         \n",
    "    return list_top\n",
    "        \n",
    "df['simila']=df['career'].apply(simila )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "#### Using one hot encoding to labels categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [  'Nom','won','non' ]\n",
    "labels =    pd.DataFrame(  columns=columns)\n",
    "\n",
    "\n",
    "labels[[ 'Nom','won','non']] =  pd.DataFrame(pd.get_dummies(df['Oscar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non</th>\n",
       "      <th>Nomwon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   non  Nomwon\n",
       "1    1       0\n",
       "2    1       0\n",
       "3    0       1\n",
       "4    1       0\n",
       "5    1       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['Nomwon']= labels['Nom'] + labels['won'] \n",
    "labels=labels.drop(['Nom', 'won'], axis=1)\n",
    "labels.head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "#### Each unique word in the 'bag of words' gets an integer. That will be used to encode the text in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "cv=CountVectorizer(max_df=0.85, max_features=100000)\n",
    "\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "sen_len = 25\n",
    "features = np.zeros((len(df), seq_len), dtype=int)\n",
    "career_ints = []\n",
    "sent_len = []\n",
    "for i, parag in enumerate(df['simila']  ):\n",
    "    for ii , sent in enumerate(parag) :\n",
    "         \n",
    "        if (sent !='none' and sent !='a' and  sent !='nan' and sent !=''):\n",
    "              \n",
    "            caree_var=[cv.vocabulary_[word] for word in sent.split()\n",
    "                                 if word in cv.vocabulary_.keys() ][0:25]\n",
    "            sent_len.append(len([cv.vocabulary_[word] for word in sent.split()\n",
    "                                 if word in cv.vocabulary_.keys() ]))  \n",
    "            features[i, ii*sen_len :ii*sen_len+len(caree_var)] = caree_var\n",
    "             \n",
    "        else:\n",
    "            caree_var=[cv.vocabulary_[word] for word in sent.split()\n",
    "                                 if word in cv.vocabulary_.keys() ][0:25]\n",
    "        career_ints.append(features[i,:])    \n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 word-length sentance: 2\n",
      "Maximum review sentance: 155\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([ x  for x in sent_len])\n",
    "print(\"1 word-length sentance: {}\".format(review_lens[1]))\n",
    "print(\"Maximum review sentance: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.344267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.683750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  3384.000000\n",
       "mean     18.344267\n",
       "std      13.683750\n",
       "min       0.000000\n",
       "25%      10.000000\n",
       "50%      16.000000\n",
       "75%      24.000000\n",
       "max     155.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "#pd.DataFrame.from_dict(  review_lens ) \n",
    "review_le = ([ x for x in sent_len])\n",
    "review_df = pd.DataFrame(review_le)\n",
    "\n",
    "review_df.hist(bins = 20) \n",
    "pl.xlabel('Number of words')\n",
    "pl.ylabel('Number of samples')\n",
    "pl.title('distribution of Number of words in sentance')\n",
    "pd.DataFrame(review_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for the model\n",
    "### Training, Validation, Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(719, 100) \n",
      "Validation set: \t(90, 100) \n",
      "Test set: \t\t(90, 100) \n",
      "Test set: \t\t(90, 2) \n",
      "Validation set: \t(90, 2) \n",
      "Train set: \t\t(719, 2)\n"
     ]
    }
   ],
   "source": [
    "#Can be replace with train test split\n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels.values[:split_idx ], labels.values[split_idx: ]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_y.shape),\n",
    "     \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "     \"\\nTrain set: \\t\\t{}\".format(train_y.shape))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph\n",
    "\n",
    "### Hyperparameters Defining.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. \n",
    "* `lstm_layers`: Number of LSTM layers in the network.\n",
    "* `batch_size`: The number of articles to feed the network in one training pass.  \n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in 300 words long vectors. Each batch will be batch_size vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. We need to do this because it is inefficient to one-hot encode our classes here. Instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. It is also possible to train an embedding layer using word2vec, then load it here. \n",
    "\n",
    "\n",
    "### LSTM cell\n",
    "\n",
    " \n",
    "Next, we'll create our LSTM cells to use in the recurrent network ([TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)). \n",
    "\n",
    "To create a basic LSTM cell for the graph, we'll   use `tf.contrib.rnn.BasicLSTMCell`.\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)\n",
    "```\n",
    "\n",
    "BasicLSTMCell   takes a parameter `num_units`, the number of units in the cell (`lstm_size`).  \n",
    "\n",
    " \n",
    "\n",
    "Next, we add dropout to the cell with `tf.contrib.rnn.DropoutWrapper`. This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make the network better with almost no effort! \n",
    "\n",
    "\n",
    "The network will have better performance with more layers. Adding more layers allows the network to learn complex relationships. A simple way to create multiple layers of LSTM cells with `tf.contrib.rnn.MultiRNNCell`:\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "```\n",
    "\n",
    "Here, `[drop] * lstm_layers` creates a list of cells (`drop`) that is `lstm_layers` long. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "So the final cell we're using in the network is actually multiple  LSTM cells with dropout. But it all works the same from an architectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    " \n",
    " \n",
    "### RNN forward pass\n",
    "Now we need to actually run the data through the RNN nodes. We use tf.nn.dynamic_rnn to do this. We pass in the RNN cell you created (our multiple layered LSTM cell for instance), and the inputs to the network.\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "Above I created an initial state, initial_state, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. tf.nn.dynamic_rnn takes care of most of the work for us. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer.\n",
    "\n",
    "\n",
    "### Output\n",
    "#### Training Cost\n",
    "We only care about the final output, we'll be using that as our prediction. So we need to grab the last output with `outputs[:, -1]`, the calculate the cost from that and `labels_`. Since we have more than 2 labels it is important to use the softmax_cross_entropy.\n",
    "\n",
    "#### Validation accuracy\n",
    "Here we   calculate the accuracy which I'll use in the validation pass.\n",
    "\n",
    "#### Batching\n",
    "This function returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the x and y arrays and returns slices out of those arrays with size [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "lstm_size = 256\n",
    "#num_units = [128, 64 ]\n",
    "#lstm_layers = 2\n",
    "num_units = [128 ]\n",
    "lstm_layers = 1\n",
    "batch_size = 30\n",
    "learning_rate = 0.001\n",
    "NUM_CLASSES = 2\n",
    "epochs = 3\n",
    "keep_prob = 0.5\n",
    "embed_size = 50  \n",
    "#seq_len = 80\n",
    "n_words = len(cv.vocabulary_)+ 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "with graph.as_default():\n",
    "    # Basic LSTM cell\n",
    " \n",
    "    lstm = [ tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(num_units=n), \n",
    "                                           output_keep_prob=keep_prob) for n in num_units]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(lstm)\n",
    "  \n",
    " \n",
    "\n",
    "    initial_state = cell.zero_state(  batch_size , tf.float32)\n",
    "# RNN forward pass\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)\n",
    "    \n",
    "# Output Training Cost\n",
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], NUM_CLASSES, \n",
    "                                                    activation_fn=tf.sigmoid)\n",
    "    # For multi output\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_, logits=predictions))\n",
    "    # For binarry output \n",
    "    # cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "# Validation, \n",
    "with graph.as_default():\n",
    "    # For binarry output \n",
    "    # correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # For multi output  \n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels_, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n",
    "    \n",
    "# Batching \n",
    "def get_batches(x, y, batch_size=batch_size):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below is the training code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Iteration: 5 Train loss: 0.686\n",
      "Epoch: 0/3 Iteration: 10 Train loss: 0.510\n",
      "Epoch: 0/3 Iteration: 15 Train loss: 0.761\n",
      "Epoch: 0/3 Iteration: 20 Train loss: 0.716\n",
      "Epoch: 1/3 Iteration: 25 Train loss: 0.610\n",
      "Val acc: 0.656\n",
      "Epoch: 1/3 Iteration: 30 Train loss: 0.633\n",
      "Epoch: 1/3 Iteration: 35 Train loss: 0.687\n",
      "Epoch: 1/3 Iteration: 40 Train loss: 0.610\n",
      "Epoch: 1/3 Iteration: 45 Train loss: 0.635\n",
      "Epoch: 2/3 Iteration: 50 Train loss: 0.547\n",
      "Val acc: 0.711\n",
      "Epoch: 2/3 Iteration: 55 Train loss: 0.688\n",
      "Epoch: 2/3 Iteration: 60 Train loss: 0.678\n",
      "Epoch: 2/3 Iteration: 65 Train loss: 0.633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, :],\n",
    "                    keep_prob: 0.7,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:,:],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "             \n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
